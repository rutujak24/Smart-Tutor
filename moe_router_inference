#!/usr/bin/env python3
# ============================================================
# Mixture-of-Experts (MoE) Inference ‚Äî Final Silent & Safe Build
# Author: Ramanathan Swaminathan (Georgia Tech)
# ============================================================

import os, json, torch, time, sys, io, contextlib
from colorama import Fore, Style, init
init(autoreset=True)

# -------------------- Silent Offline Environment --------------------
os.environ["HF_HOME"] = "/home/hice1/rswaminathan38/scratch/CS-6200-Project_ram/hf_cache"
os.environ["TRANSFORMERS_CACHE"] = os.environ["HF_HOME"]
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["UNSLOTH_OFFLINE"] = "1"
os.environ["UNSLOTH_SILENT"] = "1"
os.environ["UNSLOTH_DISABLE_LOGS"] = "1"
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

# -------------------- Imports --------------------
from unsloth import FastLanguageModel
from transformers import DistilBertTokenizer, DistilBertConfig, AutoTokenizer
from distilbert_router import DistilBertRouter, RouterInference

# ============================================================
# Paths and Config
# ============================================================
BASE = "/home/hice1/rswaminathan38/scratch/CS-6200-Project_ram"
CACHE = os.path.join(BASE, "hf_cache")
ROUTER_PATH = os.path.join(BASE, "router_model_moe_3class")
device = "cuda" if torch.cuda.is_available() else "cpu"

BASE_MODEL = os.path.join(
    CACHE,
    "models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit",
    "snapshots",
    "f15c379fb32bb402fa06a7ae9aecb1febf4b79ec",
)

CONFIDENCE_THRESHOLD = 0.70

EXPERTS = {
    "math": {"model": os.path.join(CACHE, "math_model"), "tokenizer": os.path.join(CACHE, "math_tokenizer")},
    "science": {"model": os.path.join(CACHE, "science_model"), "tokenizer": os.path.join(CACHE, "science_tokenizer")},
    "general": {"model": os.path.join(CACHE, "general_model"), "tokenizer": os.path.join(CACHE, "general_tokenizer")},
    "combined": {"model": os.path.join(CACHE, "combined_model"), "tokenizer": os.path.join(CACHE, "combined_tokenizer")},
}
label_to_expert = {"math": "math", "science": "science", "general": "general"}

# ============================================================
# Load Router
# ============================================================
print(Fore.CYAN + "üß≠ Loading DistilBERT router model...")
cfg = DistilBertConfig.from_pretrained("distilbert-base-uncased")
router_model = DistilBertRouter.load_pretrained(ROUTER_PATH, cfg, device=device)
router_tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

with open(os.path.join(ROUTER_PATH, "router_config.json")) as f:
    meta = json.load(f)
router_label_map = {str(i): name for i, name in enumerate(meta["category_names"])}

print(Fore.GREEN + "‚úÖ Router model loaded successfully.")
print(Fore.YELLOW + f"üß© Label mapping: {router_label_map}")
print(Fore.GREEN + "‚úÖ Experts configured.\n")

router = RouterInference(router_model, router_tokenizer, device=device)

# ============================================================
# Prompt Styles
# ============================================================
PROMPT_STYLES = {
    "math": "Show reasoning step by step and provide the final numeric answer at the end.",
    "science": "Explain clearly in scientific language using 2‚Äì4 sentences.",
    "general": "Answer concisely and naturally for a general audience.",
    "combined": "Provide a factual and balanced explanation suitable for any topic.",
}

# ============================================================
# Expert Loader (Silent + Safe GPU/CPU Fallback)
# ============================================================
_loaded_experts = {}

def load_expert(model_dir, tokenizer_dir, expert_key):
    if expert_key in _loaded_experts:
        print(Fore.YELLOW + f"‚ö° Using cached expert ‚Üí {expert_key}")
        return _loaded_experts[expert_key]

    devnull = open(os.devnull, "w")
    stdout_backup, stderr_backup = sys.stdout, sys.stderr
    sys.stdout = sys.stderr = devnull
    try:
        try:
            # Attempt GPU with CPU offload
            base_model, _ = FastLanguageModel.from_pretrained(
                model_name=BASE_MODEL,
                dtype=torch.float16,
                load_in_4bit=False,
                trust_remote_code=True,
                local_files_only=True,
                resume_download=False,
                device_map="auto",
                llm_int8_enable_fp32_cpu_offload=True,
            )
            base_model.load_adapter(model_dir)
            base_model = FastLanguageModel.for_inference(base_model)
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, local_files_only=True)
        except Exception:
            # Fallback: CPU only (fully silent)
            base_model, _ = FastLanguageModel.from_pretrained(
                model_name=BASE_MODEL,
                dtype=torch.float32,
                trust_remote_code=True,
                local_files_only=True,
                resume_download=False,
                device_map="cpu",
            )
            base_model.load_adapter(model_dir)
            base_model = FastLanguageModel.for_inference(base_model)
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, local_files_only=True)
    finally:
        sys.stdout, sys.stderr = stdout_backup, stderr_backup
        devnull.close()

    _loaded_experts[expert_key] = (base_model, tokenizer)
    print(Fore.GREEN + f"‚úÖ Expert ready: {expert_key}")
    return base_model, tokenizer

# ============================================================
# Logging
# ============================================================
LOG_PATH = os.path.join(BASE, "results", "moe_inference_log.jsonl")
os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)

def log_decision(query, category, conf, expert_key):
    with open(LOG_PATH, "a") as f:
        f.write(json.dumps({
            "query": query,
            "router_category": category,
            "router_confidence": conf,
            "expert_used": expert_key,
        }) + "\n")

# ============================================================
# Interactive Inference Loop
# ============================================================
print(Fore.CYAN + "üí° Router Inference Active. Type a query or 'exit' to quit.\n")

while True:
    try:
        query = input(Fore.WHITE + ">> ").strip()
        if not query or query.lower() == "exit":
            print(Fore.RED + "üõë Exiting interactive session.")
            break

        # 1Ô∏è‚É£ Route
        route = router.route_query(query)
        category, conf = route["category"], route["confidence"]
        print(Fore.MAGENTA + f"üîç Router prediction ‚Üí {category} (conf={conf:.3f})")

        # 2Ô∏è‚É£ Select expert
        expert_key = label_to_expert.get(category) if conf >= CONFIDENCE_THRESHOLD else "combined"
        expert = EXPERTS.get(expert_key, EXPERTS["combined"])

        # 3Ô∏è‚É£ Load expert (silent)
        model, tokenizer = load_expert(expert["model"], expert["tokenizer"], expert_key)
        if model is None:
            print(Fore.YELLOW + "‚ö†Ô∏è Expert load failed ‚Äî using combined fallback.\n")
            model, tokenizer = load_expert(EXPERTS["combined"]["model"], EXPERTS["combined"]["tokenizer"], "combined")
            if model is None:
                continue

        # 4Ô∏è‚É£ Prompt
        style = PROMPT_STYLES.get(expert_key, "")
        prompt = f"{query.strip()}\n\n{style}"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # 5Ô∏è‚É£ Generate (silent)
        start = time.time()
        devnull = open(os.devnull, "w")
        stdout_backup, stderr_backup = sys.stdout, sys.stderr
        sys.stdout = sys.stderr = devnull
        try:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=200,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.1,
                )
        finally:
            sys.stdout, sys.stderr = stdout_backup, stderr_backup
            devnull.close()
        elapsed = time.time() - start

        # 6Ô∏è‚É£ Decode & Clean
        text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = text.replace(prompt, "").strip()

        print(Fore.GREEN + f"\nü§ñ Expert ({expert_key}) reply:")
        print(Style.BRIGHT + response + "\n")
        print(Fore.CYAN + f"‚è±Ô∏è Inference time: {elapsed:.2f}s\n" + "-" * 70 + "\n")

        log_decision(query, category, conf, expert_key)

    except KeyboardInterrupt:
        print(Fore.RED + "\nüõë Session terminated by user.")
        break
    except Exception as e:
        print(Fore.RED + f"\n‚ùå Runtime Error: {e}\n")
        continue
